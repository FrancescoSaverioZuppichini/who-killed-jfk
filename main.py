import aiofiles
from dotenv import load_dotenv
from os import environ
from pathlib import Path
from crud import add_documents_from_links, get_db, get_documents
from logger import logger
from scrape import download_documents, scrape_documents_links
import asyncio

from utils import create_csv

load_dotenv()

SQLITE_PATH = environ["SQLITE_PATH"]
FILES_DIR = environ["FILES_DIR"]
LINKS_TEXT_PATH = environ["LINKS_TEXT_PATH"]

files_dir = Path(FILES_DIR)
files_dir.mkdir(exist_ok=True, parents=True)


async def main():
    if Path(LINKS_TEXT_PATH).exists():
        logger.info("üì¶ links file found, loading ...")
        async with aiofiles.open(LINKS_TEXT_PATH, mode="r") as f:
            links = await f.readlines()
    else:
        links = await scrape_documents_links()
        async with aiofiles.open(LINKS_TEXT_PATH, mode="w") as f:
            await f.write("\n".join(links))

    logger.info(f"ü§ñ scraped {len(links)} links.")
    db = await get_db(SQLITE_PATH)
    await add_documents_from_links(db, links)
    logger.info(f"‚úÖ added to the database")
    await create_csv("data.csv", db)
    logger.info(f"‚úÖ created .csv")
    documents = await get_documents(db)
    logger.info(f"‚¨áÔ∏è dowloading {len(documents)} documents")
    await download_documents([doc["href"] for doc in documents], out_dir=FILES_DIR)
    await db.close()
    return


if __name__ == "__main__":
    asyncio.run(main())
